{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the value function of policy\n",
    "import numpy as np\n",
    "\n",
    "# display output\n",
    "from random import uniform\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):        # return initial state\n",
    "        return grid.states[gridSize*gridSize-1] # ? self.states\n",
    "   \n",
    "    def transition_reward(self, current_pos, action): # return the transition probability\n",
    "\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, receive + 10\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, receive + 5\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "\n",
    "        # if taking an action crosses the border; agent's new_pos is the same as the current pos\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "\n",
    "        return self.new_pos, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get initial state (bottom right)\n",
    "grid.initial_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a random policy\n",
    "random_policy = np.random.randint(1000, size=(state_count, action_count))\n",
    "random_policy = random_policy/random_policy.sum(axis=1)[:,None]\n",
    "policy = random_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38041667, 0.39083333, 0.03916667, 0.18958333],\n",
       "       [0.05014191, 0.46736045, 0.09176916, 0.39072848],\n",
       "       [0.26320611, 0.26748092, 0.25648855, 0.21282443],\n",
       "       [0.34566929, 0.29448819, 0.12637795, 0.23346457],\n",
       "       [0.15637394, 0.39433428, 0.07422096, 0.37507082],\n",
       "       [0.18901454, 0.18309101, 0.34841142, 0.27948304],\n",
       "       [0.37226579, 0.29137433, 0.14156005, 0.19479983],\n",
       "       [0.39586028, 0.20137991, 0.34928849, 0.05347132],\n",
       "       [0.36378335, 0.18108327, 0.19401778, 0.2611156 ],\n",
       "       [0.01147626, 0.24308816, 0.35367762, 0.39175796],\n",
       "       [0.26828087, 0.12445521, 0.48184019, 0.12542373],\n",
       "       [0.        , 0.07354346, 0.25023878, 0.67621777],\n",
       "       [0.2837345 , 0.35010941, 0.26477024, 0.10138585],\n",
       "       [0.50028265, 0.10457886, 0.30525721, 0.08988129],\n",
       "       [0.33645346, 0.2324493 , 0.0400416 , 0.39105564],\n",
       "       [0.18852459, 0.10403531, 0.44199243, 0.26544767],\n",
       "       [0.11250983, 0.27458694, 0.3859166 , 0.22698662],\n",
       "       [0.37138085, 0.09242762, 0.07461024, 0.46158129],\n",
       "       [0.15281593, 0.30494505, 0.30357143, 0.23866758],\n",
       "       [0.34384164, 0.04032258, 0.35117302, 0.26466276],\n",
       "       [0.22184466, 0.46796117, 0.07330097, 0.2368932 ],\n",
       "       [0.41256281, 0.31356784, 0.18994975, 0.0839196 ],\n",
       "       [0.18123238, 0.10189287, 0.34716069, 0.36971406],\n",
       "       [0.14304291, 0.5396619 , 0.1579974 , 0.15929779],\n",
       "       [0.24975418, 0.33628319, 0.18387414, 0.2300885 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random policy\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Episode following policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(steps):\n",
    "\n",
    "    # set initial state\n",
    "    state = grid.initial_state()\n",
    "\n",
    "    # initialize state (with iniitial state), action list and reward list\n",
    "    state_list = [state]\n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "\n",
    "    # generate an episode\n",
    "    for i in range(steps):\n",
    "\n",
    "        # pick an action based on categorical distribution in policy\n",
    "        action = int(np.random.choice(action_count, 1, p=policy[grid.states.index(state)])) # get index in int\n",
    "        action = actions[action] # convert the integer index (ie. 0) to action (ie. [-1, 0])\n",
    "\n",
    "        # get reward (integer value)\n",
    "        reward = grid.reward(state, action)\n",
    "\n",
    "        # get the new state with the chosen action\n",
    "        new_state = list(grid.p_transition(state, action)) # (ie. [1,0])\n",
    "        state = new_state # set the next state as the current state\n",
    "\n",
    "        # save state, action chosen and reward to list\n",
    "        state_list.append(state)\n",
    "        action_list.append(action)\n",
    "        reward_list.append(reward)\n",
    "        \n",
    "    return state_list, action_list, reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA(Lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[496, 639, 662, 911],\n",
       "       [832, 815,  31, 849],\n",
       "       [831, 851, 231, 657],\n",
       "       [701, 334,   1, 774],\n",
       "       [190, 555, 741, 142],\n",
       "       [603, 433, 207,  23],\n",
       "       [945,  74, 536, 121],\n",
       "       [643, 878, 840, 521],\n",
       "       [912, 163, 805, 413],\n",
       "       [562,  50, 226, 912],\n",
       "       [528, 711, 392, 406],\n",
       "       [112, 130, 818, 689],\n",
       "       [818, 113,  80, 425],\n",
       "       [513, 735, 885, 385],\n",
       "       [998, 984, 749, 695],\n",
       "       [628, 802, 891,  94],\n",
       "       [840, 846,  26, 183],\n",
       "       [917,  83, 653,  43],\n",
       "       [568, 837, 744, 997],\n",
       "       [ 26,  15,  27,  77],\n",
       "       [ 53, 460, 740, 778],\n",
       "       [824,   3, 406, 913],\n",
       "       [943,  72, 245, 666],\n",
       "       [863, 794, 995, 153],\n",
       "       [110, 198, 300, 123]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize q values for all state action pairs\n",
    "Q_values = np.random.randint(0,1000,size = (state_count, action_count))\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize parameters\n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "lamda = 0.9\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate 500 times: each time, generating an episode of 200 steps\n",
    "max_steps = 200\n",
    "\n",
    "# define variables for keeping track of time steps\n",
    "Terminal = max_steps\n",
    "t_list=[]\n",
    "for i in range(1,max_steps+1):\n",
    "    t = Terminal - i\n",
    "    t_list.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon):\n",
    "    \n",
    "    # choose an action type: explore or exploit\n",
    "    action_type = int(np.random.choice(2, 1, p=[epsilon,1-epsilon]))\n",
    "\n",
    "    # find best action based on Q values\n",
    "    best_action_index = np.argmax(Q_values[state])\n",
    "\n",
    "    # choose an action based on exploit or explore\n",
    "    if action_type == 0:\n",
    "        # explore\n",
    "        # print(\"explore\")\n",
    "        \n",
    "        # pick a random action\n",
    "        random_action_index = np.random.choice(range(4))\n",
    "        \n",
    "        # while random action is the same as the best action, pick a new action\n",
    "        while random_action_index == best_action_index:\n",
    "            random_action_index = np.random.choice(range(4))\n",
    "        \n",
    "        action_index = random_action_index\n",
    "    else:\n",
    "        # exploit\n",
    "        # print(\"exploit\")\n",
    "        action_index = best_action_index\n",
    "        \n",
    "    return action_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_type = int(np.random.choice(2, 1, p=[epsilon,1-epsilon]))\n",
    "action_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration 500 times\n",
    "for iteration in range(500):\n",
    "    \n",
    "    # initialize delta\n",
    "    delta = 0\n",
    "    \n",
    "    # initialize S,A (? should i choose an Action using epsilon-greedy here or just select an Action?)\n",
    "    state_vector = grid.initial_state()\n",
    "    state_index = grid.states.index(state_vector)\n",
    "    \n",
    "    # initialize  eligibility traces for all state action pairs of all states to 0\n",
    "    z_values = np.zeros((state_count, action_count))\n",
    "    \n",
    "    action_index = choose_action(state_index, epsilon)\n",
    "    action_vector = actions[action_index]\n",
    "    \n",
    "    # iteration 200 steps of the episode\n",
    "    for i in range(max_steps):\n",
    "        \n",
    "        # Take action A, oberserve R, S'\n",
    "        next_state_vector, reward = grid.transition_reward(state_vector, action_vector)\n",
    "        next_state_index = grid.states.index(list(next_state_vector))\n",
    "        \n",
    "        # Choose A' from S' using policy derived from Q (eg. epsilon-greedy)\n",
    "        next_action_index = choose_action(next_state_index, epsilon)\n",
    "        next_action_vector = actions[next_action_index]\n",
    "        \n",
    "        # update the action-value form of the TD error\n",
    "        delta = reward + gamma*Q_values[next_state_index][next_action_index] - Q_values[state_index][action_index]\n",
    "        \n",
    "        # accumulate traces (? big S and big A?)\n",
    "        z_values[state_index][action_index] +=1\n",
    "        \n",
    "        # update Q value\n",
    "        Q_values[state_index][action_index] = Q_values[state_index][action_index] + alpha*delta*z_values[state_index][action_index]\n",
    "        \n",
    "        # update z value\n",
    "        z_values[state_index][action_index] = gamma*lamda*z_values[state_index][action_index]\n",
    "        \n",
    "        # update state and action vector\n",
    "        state_vector = list(next_state_vector)\n",
    "        state_index = grid.states.index(state_vector)\n",
    "        action_vector = list(next_action_vector)\n",
    "        action_index = next_action_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  9,  1,  0],\n",
       "       [12, 10, 10, 10],\n",
       "       [ 3,  4,  0,  9],\n",
       "       [12,  5,  6,  5],\n",
       "       [ 2,  1,  0,  4],\n",
       "       [ 6,  4,  0,  0],\n",
       "       [ 9,  2,  0,  1],\n",
       "       [ 5,  0,  0,  2],\n",
       "       [ 6,  0,  0,  1],\n",
       "       [ 1,  0,  0,  0],\n",
       "       [ 4,  0,  0,  0],\n",
       "       [ 1,  0,  0,  1],\n",
       "       [ 2,  0,  0,  1],\n",
       "       [ 6,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0],\n",
       "       [ 7,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0],\n",
       "       [ 1,  0,  1,  0],\n",
       "       [ 0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Table: \n",
      "       0      1      2     3     4\n",
      "0   left   left  right  left  down\n",
      "1     up     up  right    up  left\n",
      "2  right   down     up  down    up\n",
      "3   down  right     up  left  left\n",
      "4   left   left     up  down  down\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PRINT POLICY TABLE ################################################################################\n",
    "# import pandas library\n",
    "import pandas as pd\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)\n",
    "\n",
    "# iterate through policy to make a table that represents action number\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(len(Q_values)):\n",
    "    \n",
    "    # find the best action at each state\n",
    "    best_action = np.argmax(Q_values[state])\n",
    "\n",
    "    # get action name\n",
    "    if best_action == 0:\n",
    "        action_name = 'up'\n",
    "    elif best_action == 1:\n",
    "        action_name = 'right'\n",
    "    elif best_action == 2:\n",
    "        action_name = 'down'\n",
    "    else:\n",
    "        action_name = 'left'\n",
    "\n",
    "    # calculate the row and column coordinate of the current state number\n",
    "    row = int(state/grid.size)\n",
    "    column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "            \n",
    "    # assign action name\n",
    "    policy_table.loc[row][column] = action_name\n",
    "\n",
    "print(\"Policy Table: \")\n",
    "print(policy_table)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
