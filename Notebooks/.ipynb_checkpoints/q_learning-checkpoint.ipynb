{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# display output\n",
    "from random import uniform\n",
    "import random\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):        # return initial state\n",
    "        return grid.states[gridSize*gridSize-1]\n",
    "   \n",
    "    def transition_reward(self, current_pos, action): # return the transition probability\n",
    "\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, receive + 10\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, receive + 5\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "\n",
    "        # if taking an action crosses the border; agent's new_pos is the same as the current pos\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "\n",
    "        return self.new_pos, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize q values for all state action pairs\n",
    "Q_values = np.zeros((state_count, action_count))\n",
    "\n",
    "# initialize other parameters\n",
    "epsilon = 0.2\n",
    "gamma = 0.99\n",
    "lr = 0.1\n",
    "# state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon):\n",
    "    \n",
    "    # choose an action type: explore or exploit\n",
    "    action_type = int(np.random.choice(2, 1, p=[epsilon,1-epsilon]))\n",
    "\n",
    "    # find best action based on Q values\n",
    "    best_action = np.argmax(Q_values[state])\n",
    "\n",
    "    # pick a random action\n",
    "    random_action = random.choice(range(4))\n",
    "\n",
    "    # while random action is the same as the best action, pick a new action\n",
    "    while random_action == best_action:\n",
    "        random_action = random.choice(range(4))\n",
    "\n",
    "    # choose an action based on exploit or explore\n",
    "    if action_type == 0:\n",
    "        # explore\n",
    "        action = random_action\n",
    "    else:\n",
    "        # exploit\n",
    "        action = best_action\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # randomly generate Q values (for testing purpose)\n",
    "# Q_values = np.random.randint(10, size=(state_count, action_count))\n",
    "# Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(500):\n",
    "\n",
    "    # initialize state\n",
    "    state = grid.initial_state()\n",
    "\n",
    "    for step in range(200):\n",
    "\n",
    "        # get state index\n",
    "        state_index = grid.states.index(state)\n",
    "\n",
    "        # choose an action based on epsilon-greedy\n",
    "        chosen_action = choose_action(state_index, epsilon)\n",
    "\n",
    "        # convert chosen action to its vector representation\n",
    "        action_vector = actions[chosen_action]\n",
    "\n",
    "        # get the next state and reward after taking the chosen action in the current state\n",
    "        next_state, reward = grid.transition_reward(state, action_vector)\n",
    "\n",
    "        # get next_state's index\n",
    "        next_state_index = grid.states.index(list(next_state))\n",
    "\n",
    "        # update Q value\n",
    "        Q_values[state_index][chosen_action] = Q_values[state_index][chosen_action] + lr*(reward + gamma*np.max(Q_values[next_state_index])-Q_values[state_index][chosen_action])\n",
    "\n",
    "        # set the next state as the current state\n",
    "        state = list(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.31,   4.85,  33.24,   2.47],\n",
       "       [ 17.92,  33.76,  35.2 , 161.41],\n",
       "       [150.44, 166.66, 150.35, 116.1 ],\n",
       "       [168.34, 168.34, 168.34, 168.34],\n",
       "       [ 92.1 ,  89.09, 163.34, 118.94],\n",
       "       [  4.76, 142.26,  11.37,  16.65],\n",
       "       [138.58, 133.08, 160.09,  91.61],\n",
       "       [164.99, 164.99, 161.71, 158.49],\n",
       "       [166.66, 163.34, 163.34, 163.34],\n",
       "       [161.71, 162.34, 161.71, 164.99],\n",
       "       [ 40.21, 157.71,   0.  ,  -0.22],\n",
       "       [154.86, 161.71, 154.38, 147.85],\n",
       "       [163.34, 163.34, 160.09, 160.09],\n",
       "       [164.99, 161.71, 161.71, 161.71],\n",
       "       [163.34, 160.71, 160.09, 163.34],\n",
       "       [ 12.9 ,  90.46,   0.  ,   8.11],\n",
       "       [160.09,  95.61,  87.68,  30.33],\n",
       "       [161.71, 155.88, 134.61, 149.26],\n",
       "       [163.34, 160.09, 160.09, 160.09],\n",
       "       [161.05, 159.02, 158.05, 161.71],\n",
       "       [  0.  ,  77.71,   3.69,   0.  ],\n",
       "       [157.88,  76.6 ,  47.52,  22.6 ],\n",
       "       [158.38,  24.78,  39.68,  26.73],\n",
       "       [161.71, 127.1 , 125.64,  99.38],\n",
       "       [160.09, 154.94, 154.14, 158.01]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Table: \n",
      "       0      1      2   3     4\n",
      "0   down   left  right  up  down\n",
      "1  right   down  right  up  left\n",
      "2  right  right     up  up  left\n",
      "3  right     up     up  up  left\n",
      "4  right     up     up  up    up\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PRINT POLICY TABLE ################################################################################\n",
    "# import pandas library\n",
    "import pandas as pd\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)\n",
    "\n",
    "# iterate through policy to make a table that represents action number\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(len(Q_values)):\n",
    "    \n",
    "    # find the best action at each state\n",
    "    best_action = np.argmax(Q_values[state])\n",
    "\n",
    "    # get action name\n",
    "    if best_action == 0:\n",
    "        action_name = 'up'\n",
    "    elif best_action == 1:\n",
    "        action_name = 'right'\n",
    "    elif best_action == 2:\n",
    "        action_name = 'down'\n",
    "    else:\n",
    "        action_name = 'left'\n",
    "\n",
    "    # calculate the row and column coordinate of the current state number\n",
    "    row = int(state/grid.size)\n",
    "    column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "            \n",
    "    # assign action name\n",
    "    policy_table.loc[row][column] = action_name\n",
    "\n",
    "print(\"Policy Table: \")\n",
    "print(policy_table)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
