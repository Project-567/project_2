{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld with First Visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the value function of policy\n",
    "import numpy as np\n",
    "\n",
    "# display output\n",
    "from random import uniform\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.pos_check = [0, 0] # a copy of new position\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):\n",
    "        # return initial state\n",
    "        return grid.states[gridSize*gridSize-1]\n",
    "       \n",
    "    def reward(self, current_pos, action):\n",
    "        # return the reward        \n",
    "        \n",
    "        # take action in current pos\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "        return reward\n",
    "    \n",
    "    def p_transition(self, current_pos, action):\n",
    "        # return the transition probability\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "        self.pos_check = self.new_pos # make a copy of new pos before being overwritten below\n",
    "\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "        return self.new_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get initial state (bottom right)\n",
    "grid.initial_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-visit MC Control "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a random policy\n",
    "random_policy = np.random.randint(1000, size=(state_count, action_count))\n",
    "random_policy = random_policy/random_policy.sum(axis=1)[:,None]\n",
    "policy = random_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1451049 , 0.04807692, 0.53146853, 0.27534965],\n",
       "       [0.32613636, 0.22575758, 0.12878788, 0.31931818],\n",
       "       [0.23265306, 0.16479592, 0.37959184, 0.22295918],\n",
       "       [0.145469  , 0.25516693, 0.25596184, 0.34340223],\n",
       "       [0.05632653, 0.25061224, 0.68571429, 0.00734694],\n",
       "       [0.32350187, 0.25      , 0.41058052, 0.0159176 ],\n",
       "       [0.25263852, 0.06134565, 0.32915567, 0.35686016],\n",
       "       [0.12819203, 0.41317671, 0.23493361, 0.22369765],\n",
       "       [0.35714286, 0.15648496, 0.05451128, 0.4318609 ],\n",
       "       [0.48404516, 0.17967599, 0.32596956, 0.01030928],\n",
       "       [0.47902098, 0.04079254, 0.1013986 , 0.37878788],\n",
       "       [0.03928571, 0.56785714, 0.28785714, 0.105     ],\n",
       "       [0.37091264, 0.19521718, 0.17520742, 0.25866276],\n",
       "       [0.13180304, 0.21436505, 0.30766383, 0.34616809],\n",
       "       [0.10808709, 0.38646967, 0.26749611, 0.23794712],\n",
       "       [0.42572883, 0.3465988 , 0.15363258, 0.0740398 ],\n",
       "       [0.38723404, 0.22978723, 0.01702128, 0.36595745],\n",
       "       [0.27117327, 0.14141414, 0.21794872, 0.36946387],\n",
       "       [0.51707892, 0.32391048, 0.098351  , 0.0606596 ],\n",
       "       [0.30200121, 0.00485143, 0.47604609, 0.21710127],\n",
       "       [0.14361702, 0.45454545, 0.10541586, 0.29642166],\n",
       "       [0.30129463, 0.28677913, 0.02706944, 0.38485681],\n",
       "       [0.28565179, 0.29833771, 0.31189851, 0.10411199],\n",
       "       [0.17099014, 0.24040921, 0.35842163, 0.23017903],\n",
       "       [0.47309536, 0.03622802, 0.06925946, 0.42141716]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random policy\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Episode following policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(steps):\n",
    "\n",
    "    # set initial state\n",
    "    state = grid.initial_state()\n",
    "\n",
    "    # initialize state (with iniitial state), action list\n",
    "    state_list = [state]\n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "\n",
    "    # generate an episode\n",
    "    for i in range(200):\n",
    "\n",
    "        # pick an action based on categorical distribution in policy\n",
    "        action = int(np.random.choice(action_count, 1, p=policy[grid.states.index(state)]))\n",
    "        action = actions[action]\n",
    "\n",
    "        # get reward\n",
    "        reward = grid.reward(state, action)\n",
    "    #     print(state)\n",
    "    #     print(action)\n",
    "    #     print(reward)\n",
    "\n",
    "        # get the new state with the chosen action\n",
    "        new_state = list(grid.p_transition(state, action))\n",
    "        state = new_state\n",
    "\n",
    "        # save state and action to list\n",
    "        state_list.append(state)\n",
    "        action_list.append(action)\n",
    "\n",
    "        # save reward to list\n",
    "        reward_list.append(reward)\n",
    "        \n",
    "    return state_list, action_list, reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize q values for all state action pairs\n",
    "Q_values = np.zeros((state_count, action_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop for each steo of episode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize parameters\n",
    "G = 0\n",
    "gamma = 0.99\n",
    "epsilon = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define average function\n",
    "def Average(lst): \n",
    "    return sum(lst) / len(lst) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(500):\n",
    "  \n",
    "    # generate an episode of specified step count\n",
    "    state_list, action_list, reward_list = generate_episode(200)\n",
    "    \n",
    "    # define variables for keeping track of time steps\n",
    "    Terminal = len(action_list)\n",
    "    t_list=[]\n",
    "    for i in range(1,Terminal+1):\n",
    "        t = Terminal - i\n",
    "        t_list.append(t)\n",
    "    \n",
    "    # define lists\n",
    "    returns_list = []\n",
    "    visited_list = []\n",
    "\n",
    "    # loop for each step of episode\n",
    "    for t in t_list:\n",
    "\n",
    "        # add to G\n",
    "        G = gamma*G + reward_list[t]\n",
    "\n",
    "        # combine state action pair\n",
    "        visited = []\n",
    "        visited.extend(state_list[t])\n",
    "        visited.extend(action_list[t])\n",
    "\n",
    "        # check if state action pair have been visited before\n",
    "        if visited not in visited_list:\n",
    "\n",
    "            # add state action pair to visited list\n",
    "            visited_list.append(visited)\n",
    "\n",
    "            # append G to returns\n",
    "            returns_list.append(G)\n",
    "\n",
    "            # find state and action index\n",
    "            state_index = grid.states.index(state_list[t])\n",
    "            action_index = actions.index(action_list[t])\n",
    "    #         print(\"state_index: \", state_index)\n",
    "\n",
    "            # write Q_values to the state-action pair\n",
    "            Q_values[state_index][action_index] = Average(returns_list)\n",
    "    #         print(\"average return: \", Average(returns_list))\n",
    "\n",
    "            # choose best action at given state\n",
    "            choose_action = np.argmax(Q_values[state_index])\n",
    "\n",
    "            # overwrite policy\n",
    "            for i in range(action_count):\n",
    "                if choose_action == i:\n",
    "                    policy[state_index][i] = 1 - epsilon + epsilon/action_count\n",
    "                else:\n",
    "                    policy[state_index][i] = epsilon/action_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-44.84718765, -51.10529407, -45.934096  , -50.16334669],\n",
       "       [-52.24112607, -47.38236652, -30.02940041,   1.30508548],\n",
       "       [-33.14474249, -59.7146156 , -38.16241718, -46.58664535],\n",
       "       [-69.09657523, -60.52054487, -56.2764614 , -60.62330343],\n",
       "       [-59.75060799, -46.08096515, -61.52929634, -59.87735349],\n",
       "       [-49.39363481, -27.54995697, -46.5143136 , -46.52596221],\n",
       "       [-61.9800096 , -55.02894877, -54.1934294 , -48.69221377],\n",
       "       [-45.84932055, -62.56751375, -38.85085286, -61.3793388 ],\n",
       "       [-60.42276026, -62.13663194, -62.45739602, -60.81496602],\n",
       "       [-60.23691834, -60.52994436, -62.04584429, -62.17108951],\n",
       "       [-26.53198205, -49.84761535, -33.82424905, -32.3266496 ],\n",
       "       [ -1.01260864, -63.14489186, -34.6419286 , -33.76160296],\n",
       "       [-45.16290856, -56.53106816, -50.08998898, -63.27936744],\n",
       "       [-62.16340003, -61.29547854, -61.26852713, -56.53633511],\n",
       "       [-61.08685018, -59.68676363, -61.18616644, -61.29383676],\n",
       "       [-17.56496536, -49.22005638, -48.24554147, -39.81561671],\n",
       "       [-34.46871896, -49.46974662, -42.07710422, -48.97299968],\n",
       "       [-44.52117181, -39.07998467, -49.14484152, -41.32628206],\n",
       "       [-61.33101413, -60.67186183, -40.57791999, -49.47979569],\n",
       "       [-60.0399964 , -60.36471343, -61.12320948, -61.7622868 ],\n",
       "       [-45.37470523, -46.86571818,   1.28390854, -48.46122406],\n",
       "       [-48.67592144, -47.02738909, -46.30720288, -47.22183364],\n",
       "       [-42.87750241, -48.9821965 , -48.68919522, -48.07202335],\n",
       "       [-49.72232284, -48.25234953, -43.76383659, -48.73728552],\n",
       "       [-61.96280092, -62.41988596, -62.74431253, -48.49400728]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85, 0.05, 0.05, 0.05],\n",
       "       [0.05, 0.05, 0.05, 0.85],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.05, 0.05, 0.85, 0.05],\n",
       "       [0.05, 0.85, 0.05, 0.05],\n",
       "       [0.05, 0.85, 0.05, 0.05],\n",
       "       [0.05, 0.05, 0.05, 0.85],\n",
       "       [0.05, 0.05, 0.85, 0.05],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.05, 0.05, 0.05, 0.85],\n",
       "       [0.05, 0.85, 0.05, 0.05],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.05, 0.85, 0.05, 0.05],\n",
       "       [0.05, 0.05, 0.85, 0.05],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.05, 0.05, 0.85, 0.05],\n",
       "       [0.05, 0.05, 0.85, 0.05],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.05, 0.05, 0.85, 0.05],\n",
       "       [0.05, 0.05, 0.05, 0.85]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#up, right, down, left = (clockwise from up) \n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Table: \n",
      "       0     1      2     3      4\n",
      "0     up  left     up  down  right\n",
      "1  right  left   down    up     up\n",
      "2     up    up     up  left  right\n",
      "3     up    up  right  down     up\n",
      "4   down  down     up  down   left\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PRINT POLICY TABLE ################################################################################\n",
    "# import pandas library\n",
    "import pandas as pd\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)\n",
    "\n",
    "# iterate through policy to make a table that represents action number\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(len(policy)):\n",
    "    \n",
    "    # find the best action at each state\n",
    "    best_action = np.argmax(policy[state])\n",
    "\n",
    "    # get action name\n",
    "    if best_action == 0:\n",
    "        action_name = 'up'\n",
    "    elif best_action == 1:\n",
    "        action_name = 'right'\n",
    "    elif best_action == 2:\n",
    "        action_name = 'down'\n",
    "    else:\n",
    "        action_name = 'left'\n",
    "\n",
    "    # calculate the row and column coordinate of the current state number\n",
    "    row = int(state/grid.size)\n",
    "    column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "            \n",
    "    # assign action name\n",
    "    policy_table.loc[row][column] = action_name\n",
    "\n",
    "print(\"Policy Table: \")\n",
    "print(policy_table)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
