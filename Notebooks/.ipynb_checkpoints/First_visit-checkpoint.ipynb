{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld with First Visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the value function of policy\n",
    "import numpy as np\n",
    "\n",
    "# display output\n",
    "from random import uniform\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.pos_check = [0, 0] # a copy of new position\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):\n",
    "        # return initial state\n",
    "        return grid.states[gridSize*gridSize-1]\n",
    "       \n",
    "    def reward(self, current_pos, action):\n",
    "        # return the reward        \n",
    "        \n",
    "        # take action in current pos\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "        return reward\n",
    "    \n",
    "    def p_transition(self, current_pos, action):\n",
    "        # return the transition probability\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "        self.pos_check = self.new_pos # make a copy of new pos before being overwritten below\n",
    "\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "        return self.new_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get initial state (bottom right)\n",
    "grid.initial_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-visit MC Control "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a random policy\n",
    "random_policy = np.random.randint(1000, size=(state_count, action_count))\n",
    "random_policy = random_policy/random_policy.sum(axis=1)[:,None]\n",
    "policy = random_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36597938, 0.00979381, 0.14742268, 0.47680412],\n",
       "       [0.68743914, 0.1781889 , 0.05160662, 0.08276534],\n",
       "       [0.04357067, 0.61742827, 0.01912859, 0.31987248],\n",
       "       [0.03361345, 0.61038961, 0.05958747, 0.29640947],\n",
       "       [0.37012987, 0.3284632 , 0.10119048, 0.20021645],\n",
       "       [0.21594684, 0.00598007, 0.37873754, 0.39933555],\n",
       "       [0.33679928, 0.3164557 , 0.16048825, 0.18625678],\n",
       "       [0.17863954, 0.21614749, 0.50603942, 0.09917355],\n",
       "       [0.44804011, 0.17639015, 0.19325433, 0.18231541],\n",
       "       [0.28622715, 0.24640992, 0.15861619, 0.30874674],\n",
       "       [0.04765507, 0.0204236 , 0.57942511, 0.35249622],\n",
       "       [0.38526316, 0.24421053, 0.09010526, 0.28042105],\n",
       "       [0.08879781, 0.40846995, 0.31215847, 0.19057377],\n",
       "       [0.13599182, 0.37116564, 0.0398773 , 0.45296524],\n",
       "       [0.11905713, 0.25209748, 0.37235318, 0.25649221],\n",
       "       [0.22997416, 0.34409991, 0.30878553, 0.1171404 ],\n",
       "       [0.41422959, 0.22697512, 0.20689655, 0.15189873],\n",
       "       [0.09981025, 0.17229602, 0.36242884, 0.3654649 ],\n",
       "       [0.19872538, 0.4797219 , 0.23986095, 0.08169177],\n",
       "       [0.47206704, 0.28324022, 0.05363128, 0.19106145],\n",
       "       [0.10824742, 0.28694158, 0.29982818, 0.30498282],\n",
       "       [0.3735934 , 0.00300075, 0.51912978, 0.10427607],\n",
       "       [0.43037037, 0.01481481, 0.26814815, 0.28666667],\n",
       "       [0.05508197, 0.6052459 , 0.1547541 , 0.18491803],\n",
       "       [0.15277778, 0.46934866, 0.28017241, 0.09770115]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random policy\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Episode following policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial state\n",
    "state = grid.initial_state()\n",
    "\n",
    "# initialize state (with iniitial state), action list\n",
    "state_list = [state]\n",
    "action_list = []\n",
    "reward_list = []\n",
    "\n",
    "# generate an episode\n",
    "for i in range(5):\n",
    "    \n",
    "    # pick an action based on categorical distribution in policy\n",
    "    action = int(np.random.choice(action_count, 1, p=policy[grid.states.index(state)]))\n",
    "    action = actions[action]\n",
    "    \n",
    "    reward = grid.reward(state, action)\n",
    "#     print(state)\n",
    "#     print(action)\n",
    "#     print(reward)\n",
    "    \n",
    "    # get the new state with the chosen action\n",
    "    new_state = list(grid.p_transition(state, action))\n",
    "    state = new_state\n",
    "   \n",
    "    # save state and action to list\n",
    "    state_list.append(state)\n",
    "    action_list.append(action)\n",
    "    \n",
    "    # save reward to list\n",
    "    reward_list.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, -1, 0, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 4], [4, 3], [4, 4], [4, 4], [3, 4], [2, 4]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, -1], [0, 1], [1, 0], [-1, 0], [-1, 0]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize q values for all state action pairs\n",
    "Q_values = np.zeros((state_count, action_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop for each steo of episode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize parameters\n",
    "G = 0\n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "Terminal = len(action_list)\n",
    "\n",
    "# define lists\n",
    "returns_list = []\n",
    "visited_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list=[]\n",
    "for i in range(1,Terminal+1):\n",
    "    t = Terminal - i\n",
    "    t_list.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3, 2, 1, 0]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define average function\n",
    "def Average(lst): \n",
    "    return sum(lst) / len(lst) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:  4\n",
      "NOT visited\n",
      "[3, 4, -1, 0]\n",
      "state_index:  19\n",
      "average return:  0.0\n",
      "t:  3\n",
      "NOT visited\n",
      "[4, 4, -1, 0]\n",
      "state_index:  24\n",
      "average return:  0.0\n",
      "t:  2\n",
      "NOT visited\n",
      "[4, 4, 1, 0]\n",
      "state_index:  24\n",
      "average return:  -0.3333333333333333\n",
      "t:  1\n",
      "NOT visited\n",
      "[4, 3, 0, 1]\n",
      "state_index:  23\n",
      "average return:  -0.4975\n",
      "t:  0\n",
      "NOT visited\n",
      "[4, 4, 0, -1]\n",
      "state_index:  24\n",
      "average return:  -0.59402\n"
     ]
    }
   ],
   "source": [
    "for t in t_list:\n",
    "    \n",
    "    print(\"t: \", t)\n",
    "    \n",
    "    # add to G\n",
    "    G = gamma*G + reward_list[t]\n",
    "    \n",
    "    # combine state action pair\n",
    "    visited = []\n",
    "    visited.extend(state_list[t])\n",
    "    visited.extend(action_list[t])\n",
    "    \n",
    "    # check if state action pair have been visited before\n",
    "    if visited in visited_list:\n",
    "        print(\"visited\")\n",
    "        print(visited)\n",
    "        \n",
    "    else:\n",
    "        print(\"NOT visited\")\n",
    "        print(visited)\n",
    "        \n",
    "        # add state action pair to visited list\n",
    "        visited_list.append(visited)\n",
    "        \n",
    "        # append G to returns\n",
    "        returns_list.append(G)\n",
    "        \n",
    "        # find state and action index\n",
    "        state_index = grid.states.index(state_list[t])\n",
    "        action_index = actions.index(action_list[t])\n",
    "        print(\"state_index: \", state_index)\n",
    "\n",
    "        # write Q_values to the state-action pair\n",
    "        Q_values[state_index][action_index] = Average(returns_list)\n",
    "        print(\"average return: \", Average(returns_list))\n",
    "    \n",
    "        # choose best action at given state\n",
    "        choose_action = np.argmax(Q_values[state_index])\n",
    "        \n",
    "        # overwrite policy\n",
    "        for i in range(action_count):\n",
    "            if choose_action == i:\n",
    "                policy[state_index][i] = 1 - epsilon + epsilon/action_count\n",
    "            else:\n",
    "                policy[state_index][i] = epsilon/action_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        , -0.4975    ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        , -0.33333333, -0.59402   ]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36597938, 0.00979381, 0.14742268, 0.47680412],\n",
       "       [0.68743914, 0.1781889 , 0.05160662, 0.08276534],\n",
       "       [0.04357067, 0.61742827, 0.01912859, 0.31987248],\n",
       "       [0.03361345, 0.61038961, 0.05958747, 0.29640947],\n",
       "       [0.37012987, 0.3284632 , 0.10119048, 0.20021645],\n",
       "       [0.21594684, 0.00598007, 0.37873754, 0.39933555],\n",
       "       [0.33679928, 0.3164557 , 0.16048825, 0.18625678],\n",
       "       [0.17863954, 0.21614749, 0.50603942, 0.09917355],\n",
       "       [0.44804011, 0.17639015, 0.19325433, 0.18231541],\n",
       "       [0.28622715, 0.24640992, 0.15861619, 0.30874674],\n",
       "       [0.04765507, 0.0204236 , 0.57942511, 0.35249622],\n",
       "       [0.38526316, 0.24421053, 0.09010526, 0.28042105],\n",
       "       [0.08879781, 0.40846995, 0.31215847, 0.19057377],\n",
       "       [0.13599182, 0.37116564, 0.0398773 , 0.45296524],\n",
       "       [0.125     , 0.625     , 0.125     , 0.125     ],\n",
       "       [0.22997416, 0.34409991, 0.30878553, 0.1171404 ],\n",
       "       [0.41422959, 0.22697512, 0.20689655, 0.15189873],\n",
       "       [0.09981025, 0.17229602, 0.36242884, 0.3654649 ],\n",
       "       [0.19872538, 0.4797219 , 0.23986095, 0.08169177],\n",
       "       [0.85      , 0.05      , 0.05      , 0.05      ],\n",
       "       [0.10824742, 0.28694158, 0.29982818, 0.30498282],\n",
       "       [0.3735934 , 0.00300075, 0.51912978, 0.10427607],\n",
       "       [0.43037037, 0.01481481, 0.26814815, 0.28666667],\n",
       "       [0.85      , 0.05      , 0.05      , 0.05      ],\n",
       "       [0.85      , 0.05      , 0.05      , 0.05      ]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = gamma*G + reward_list\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine state action pair\n",
    "visited = []\n",
    "visited.extend(state_list[-1])\n",
    "visited.extend(action_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if state action pair have been visited before\n",
    "if visited in visited_list:\n",
    "    print(\"yes\")\n",
    "    \n",
    "else:\n",
    "    print(\"no\")\n",
    "    # add state action pair to visited list\n",
    "    visited_list.append(visited)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append G to returns\n",
    "returns_list.append(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define average function\n",
    "def Average(lst): \n",
    "    return sum(lst) / len(lst) \n",
    "\n",
    "# find state and action index\n",
    "state_index = grid.states.index(state_list[-1])\n",
    "action_index = actions.index(action_list[-1])\n",
    "\n",
    "# write Q_values to the state-action pair\n",
    "Q_values[state_index][action_index] = Average(returns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get state_number\n",
    "state_number = grid.states.index(state_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_action = np.argmax(Q_values[state_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite policy\n",
    "for i in range(action_count):\n",
    "    if choose_action == i:\n",
    "        policy[state_number][i] = 1 - epsilon + epsilon/action_count\n",
    "    else:\n",
    "        policy[state_number][i] = epsilon/action_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy[state_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
