{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# display output\n",
    "from random import uniform\n",
    "import random\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):        # return initial state\n",
    "        return grid.states[gridSize*gridSize-1]\n",
    "   \n",
    "    def transition_reward(self, current_pos, action): # return the transition probability\n",
    "\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, receive + 10\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, receive + 5\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "\n",
    "        # if taking an action crosses the border; agent's new_pos is the same as the current pos\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "\n",
    "        return self.new_pos, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization of Grid\n",
    "grid = Gridworld(5)\n",
    "#Q matrix of zeros\n",
    "Q_values = np.zeros((grid.size*grid.size, len(actions)))\n",
    "# initialize other parameters\n",
    "epsilon = 0.2\n",
    "lr = 0.1\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    # choose an action type: explore or exploit\n",
    "    action_type = int(np.random.choice(2, 1, p=[(len(actions)-1)*(epsilon/len(actions)),1-(len(actions)-1)*(epsilon/len(actions))]))\n",
    "    \n",
    "    best_action_index = np.argmax(Q_values[state]) # find best action based on Q values\n",
    "    \n",
    "    action_index = best_action_index #assigns best action index to the action, this will hold only if action type is exploit\n",
    "\n",
    "    if action_type == 0: #if action type is explore, then choses a different action than argmax\n",
    "        while action_index == best_action_index: \n",
    "                action_index = int(np.random.choice(4,1))\n",
    "    return action_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished\n",
      "[[ 2.56624924e-01  2.23710608e+01  0.00000000e+00  1.89866306e+00]\n",
      " [ 5.82310328e+01  1.78448708e+01  1.63911635e+01  1.99546012e+01]\n",
      " [ 4.64254626e+01  1.02904352e+02  6.34955760e+01  3.32206281e+01]\n",
      " [ 1.10339277e+02  1.10217527e+02  1.10919336e+02  1.10397525e+02]\n",
      " [ 4.44718671e+01  2.27792250e+00  1.01998593e+02  2.15294750e+01]\n",
      " [ 9.90287289e+00  1.66715970e+00  9.76276353e-01 -9.51547077e-02]\n",
      " [ 2.86960141e+01  9.67722616e+01  2.66265532e+01  4.64866410e+00]\n",
      " [ 9.31411436e+01  1.08370863e+02  9.74102300e+01  7.91681531e+01]\n",
      " [ 1.09680905e+02  1.05050468e+02  1.05000188e+02  1.01450993e+02]\n",
      " [ 7.20504282e+01  1.02489592e+02  9.86204230e+01  1.09196971e+02]\n",
      " [ 1.52789481e+00  5.89275653e+01  7.75354808e+00  5.09868116e+00]\n",
      " [ 5.79797149e+01  1.05097046e+02  6.37558181e+01  2.93817074e+01]\n",
      " [ 1.00820183e+02  1.07250628e+02  6.71579553e+01  9.50645459e+01]\n",
      " [ 1.07799180e+02  1.03498382e+02  1.01028113e+02  1.02869130e+02]\n",
      " [ 1.01684047e+02  9.66543243e+01  8.98346617e+01  1.06320634e+02]\n",
      " [ 8.31009179e-01  5.32847340e+01  6.01305488e+00  6.83827697e+00]\n",
      " [ 8.69303107e+01  4.12520727e+01  3.69849813e+01  2.55808352e+01]\n",
      " [ 3.88992970e+01  4.39473252e+01  1.02737063e+01  7.68252451e+01]\n",
      " [ 1.06225174e+02  9.15418716e+01  8.88335735e+01  6.37316367e+01]\n",
      " [ 6.48331060e+01  5.36590058e+01  5.85896611e+01  1.01995456e+02]\n",
      " [ 7.19422816e-03  3.30645202e+01  2.28887769e+00  1.14979949e+00]\n",
      " [ 5.90922749e+01  1.45199955e+01  2.39167841e+01  1.01412145e+01]\n",
      " [ 1.22197092e+01  2.01537206e+01  1.12798987e+00  5.84985485e+01]\n",
      " [ 1.00790474e+02  7.48917778e+01  8.40670900e+01  3.77828482e+01]\n",
      " [ 7.20268061e+01  7.71034180e+01  7.79965375e+01  9.64494149e+01]]\n"
     ]
    }
   ],
   "source": [
    "for episode in range(500):\n",
    "\n",
    "    # initialize state (output: [4, 4])\n",
    "    state = grid.initial_state()\n",
    "\n",
    "    # iterate over 200 steps within each episode\n",
    "    for step in range(200):\n",
    "\n",
    "        # get state index (output: 24)\n",
    "        state_index = grid.states.index(state)\n",
    "\n",
    "        # choose an action based on epsilon-greedy (output: action index ie. 0)\n",
    "        action_index = choose_action(state_index)\n",
    "        action_vector = actions[action_index] # convert action_index (0) to action_vector ([-1, 0])\n",
    "\n",
    "        # get the next state and reward after taking the chosen action in the current state\n",
    "        next_state_vector, reward = grid.transition_reward(state, action_vector)\n",
    "        next_state_index = grid.states.index(list(next_state_vector))\n",
    "        next_action_index = choose_action(next_state_index)\n",
    "\n",
    "        # update Q value\n",
    "        Q_values[state_index][action_index] = Q_values[state_index][action_index] + lr*(reward + gamma*Q_values[next_state_index][next_action_index]-Q_values[state_index][action_index])\n",
    "\n",
    "        # set the next state as the current state\n",
    "        state = list(next_state_vector)\n",
    "print('training finished')\n",
    "print(Q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Table: \n",
      "       0      1      2     3     4\n",
      "0  right     up  right  down  down\n",
      "1     up  right  right    up  left\n",
      "2  right  right  right    up  left\n",
      "3  right     up   left    up  left\n",
      "4  right     up   left    up  left\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FIND ARGMAX POLICY \n",
    "\n",
    "import pandas as pd\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)\n",
    "\n",
    "# iterate through Q matrix to find best action\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(grid.size*grid.size):\n",
    "  \n",
    "    # find the best action at each state\n",
    "    best_action = np.argmax(Q_values[state])\n",
    "\n",
    "    # get action name\n",
    "    if best_action == 0:\n",
    "        action_name = 'up'\n",
    "    elif best_action == 1:\n",
    "        action_name = 'right'\n",
    "    elif best_action == 2:\n",
    "        action_name = 'down'\n",
    "    else:\n",
    "        action_name = 'left'\n",
    "\n",
    "    # calculate the row and column coordinate of the current state number\n",
    "    row = int(state/grid.size)\n",
    "    column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "            \n",
    "    # assign action name\n",
    "    policy_table.loc[row][column] = action_name\n",
    "\n",
    "print(\"Policy Table: \")\n",
    "print(policy_table)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
