{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from random import uniform\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):        # return initial state\n",
    "        return grid.states[gridSize*gridSize-1]\n",
    "   \n",
    "    def transition_reward(self, current_pos, action): # return the transition probability\n",
    "\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, receive + 10\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, receive + 5\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "\n",
    "        # if taking an action crosses the border; agent's new_pos is the same as the current pos\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "\n",
    "        return self.new_pos, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that chooses action based on epsilon-greedy\n",
    "def choose_action(state, epsilon):\n",
    "    \n",
    "    # choose an action type: explore or exploit\n",
    "    action_type = int(np.random.choice(2, 1, p=[epsilon,1-epsilon]))\n",
    "\n",
    "    # find best action based on Q values\n",
    "    best_action_index = np.argmax(Q_values[state])\n",
    "\n",
    "    # pick a random action\n",
    "    random_action_index = random.choice(range(4))\n",
    "\n",
    "    # while random action is the same as the best action, pick a new action\n",
    "    while random_action_index == best_action_index:\n",
    "        random_action_index = random.choice(range(4))\n",
    "\n",
    "    # choose an action based on exploit or explore\n",
    "    if action_type == 0:\n",
    "        # explore\n",
    "        # print(\"explore\")\n",
    "        action_index = random_action_index\n",
    "    else:\n",
    "        # exploit\n",
    "        # print(\"exploit\")\n",
    "        action_index = best_action_index\n",
    "        \n",
    "    return action_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define average function\n",
    "def Average(lst): \n",
    "    return sum(lst) / len(lst) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that generates an episode\n",
    "def generate_episode(steps):\n",
    "\n",
    "    # set initial state\n",
    "    state_vector = grid.initial_state()\n",
    "\n",
    "    # initialize state (with iniitial state), action list and reward list\n",
    "    state_list = [state_vector]\n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "\n",
    "    # generate an episode\n",
    "    for i in range(steps):\n",
    "\n",
    "        # pick an action based on categorical distribution in policy\n",
    "        action_index = int(np.random.choice(action_count, 1, p=policy[grid.states.index(state_vector)])) \n",
    "        action_vector = actions[action_index] # convert the integer index (ie. 0) to action (ie. [-1, 0])\n",
    "\n",
    "        # get new state and reward after taking action from current state\n",
    "        new_state_vector, reward = grid.transition_reward(state_vector, action_vector)\n",
    "        state_vector = list(new_state_vector)\n",
    "\n",
    "        # save state, action chosen and reward to list\n",
    "        state_list.append(state_vector)\n",
    "        action_list.append(action_vector)\n",
    "        reward_list.append(reward)\n",
    "        \n",
    "    return state_list, action_list, reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)\n",
    "\n",
    "# initialize q values for all state action pairs\n",
    "Q_values = np.zeros((state_count, action_count))\n",
    "\n",
    "# initialize other parameters\n",
    "gamma = 0.99\n",
    "lr = 0.1\n",
    "epsilon = 0.1\n",
    "\n",
    "# define lists for plots\n",
    "average_reward_list = []\n",
    "cumulative_reward_list = []\n",
    "cumulative_reward = 0\n",
    "delta_list = []\n",
    "episode_test_reward_list = []\n",
    "average_test_reward_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over 500 episodes\n",
    "for episode in range(500):\n",
    "\n",
    "    # initialize state (output: [4, 4])\n",
    "    state = grid.initial_state()\n",
    "\n",
    "    reward_list = []\n",
    "    delta = 0\n",
    "    \n",
    "    # iterate over 200 steps within each episode\n",
    "    for step in range(200):\n",
    "\n",
    "        # get state index (output: 24)\n",
    "        state_index = grid.states.index(state)\n",
    "\n",
    "        # choose an action based on epsilon-greedy (output: action index ie. 0)\n",
    "        action_index = choose_action(state_index, epsilon)\n",
    "        action_vector = actions[action_index] # convert action_index (0) to action_vector ([-1, 0])\n",
    "\n",
    "        # get the next state and reward after taking the chosen action in the current state\n",
    "        next_state_vector, reward = grid.transition_reward(state, action_vector)\n",
    "        next_state_index = grid.states.index(list(next_state_vector))\n",
    "\n",
    "        # add reward to list\n",
    "        reward_list.append(reward)\n",
    "        \n",
    "        # calculate max delta change for plotting max q value change\n",
    "        Q_value = Q_values[state_index][action_index] + lr*(reward + gamma*np.max(Q_values[next_state_index])-Q_values[state_index][action_index])\n",
    "        delta = max(delta, np.abs(Q_value - Q_values[state_index][action_index]))   \n",
    "        \n",
    "        # update Q value\n",
    "        Q_values[state_index][action_index] = Q_values[state_index][action_index] + lr*(reward + gamma*np.max(Q_values[next_state_index])-Q_values[state_index][action_index])\n",
    "\n",
    "        # set the next state as the current state\n",
    "        state = list(next_state_vector)\n",
    "    \n",
    "    # append max change in Q value to list\n",
    "    delta_list.append(delta)\n",
    "    \n",
    "    # average rewards\n",
    "    average_reward_list.append(Average(reward_list))\n",
    "    \n",
    "    # add cumulative reward\n",
    "    cumulative_reward = cumulative_reward + sum(reward_list)\n",
    "    cumulative_reward_list.append(cumulative_reward)\n",
    "    \n",
    "    # initialize q values for all state action pairs\n",
    "    policy = np.zeros((state_count, action_count))\n",
    "    \n",
    "    # Generate Greedy policy based on Q_values after each episode\n",
    "    for state in range(len(Q_values)):\n",
    "        # find the best action at each state\n",
    "        best_action = np.argmax(Q_values[state])\n",
    "        # write deterministic policy based on Q_values\n",
    "        policy[state][best_action] = 1\n",
    "    \n",
    "    # Generate test trajectory with the greedy policy\n",
    "    state_list, action_list, test_reward_list = generate_episode(200)\n",
    "    \n",
    "    # sum up all the rewards obtained during test trajectory and append to list\n",
    "    episode_test_reward_list.append(sum(test_reward_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Average(cumulative_test_reward_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Reward')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test reward of each episode, where delta is the change in Q values\n",
    "plt.plot(episode_test_reward_list)\n",
    "plt.title('Test Reward per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max delta of each episode, where delta is the change in Q values\n",
    "plt.plot(delta_list)\n",
    "plt.title('Max Delta per Episode during Training')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Max Delta')\n",
    "\n",
    "# plot moving average\n",
    "delta_frame = pd.DataFrame(delta_list)\n",
    "rolling_mean = delta_frame.rolling(window=10).mean()\n",
    "plt.plot(rolling_mean, label='Moving Average', color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average reward per episode\n",
    "plt.plot(average_reward_list)\n",
    "plt.title('Average Reward per Episode during Training')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "\n",
    "# plot moving average\n",
    "reward_frame = pd.DataFrame(average_reward_list)\n",
    "rolling_mean = reward_frame.rolling(window=10).mean()\n",
    "plt.plot(rolling_mean, label='Moving Average', color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative reward per episode\n",
    "plt.plot(cumulative_reward_list)\n",
    "plt.title('Cumulative Reward per Episode during Training')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "im = Image.fromarray(Q_values)\n",
    "im.save(\"your_file.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Final Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT POLICY TABLE ################################################################################\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)\n",
    "\n",
    "# iterate through policy to make a table that represents action number\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(len(Q_values)):\n",
    "    \n",
    "    # find the best action at each state\n",
    "    best_action = np.argmax(Q_values[state])\n",
    "\n",
    "    # get action name\n",
    "    if best_action == 0:\n",
    "        action_name = 'up'\n",
    "    elif best_action == 1:\n",
    "        action_name = 'right'\n",
    "    elif best_action == 2:\n",
    "        action_name = 'down'\n",
    "    else:\n",
    "        action_name = 'left'\n",
    "\n",
    "    # calculate the row and column coordinate of the current state number\n",
    "    row = int(state/grid.size)\n",
    "    column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "            \n",
    "    # assign action name\n",
    "    policy_table.loc[row][column] = action_name\n",
    "\n",
    "print(\"Policy Table: \")\n",
    "print(policy_table)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = policy_table.plot()\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('asdf.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
