{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld with First Visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the value function of policy\n",
    "import numpy as np\n",
    "\n",
    "# display output\n",
    "from random import uniform\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.pos_check = [0, 0] # a copy of new position\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):\n",
    "        # return initial state\n",
    "        return grid.states[gridSize*gridSize-1]\n",
    "       \n",
    "    def reward(self, current_pos, action):\n",
    "        # return the reward        \n",
    "        \n",
    "        # take action in current pos\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "        return reward\n",
    "    \n",
    "    def p_transition(self, current_pos, action):\n",
    "        # return the transition probability\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "        self.pos_check = self.new_pos # make a copy of new pos before being overwritten below\n",
    "\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "        return self.new_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get initial state (bottom right)\n",
    "grid.initial_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-visit MC Control "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a random policy\n",
    "random_policy = np.random.randint(1000, size=(state_count, action_count))\n",
    "random_policy = random_policy/random_policy.sum(axis=1)[:,None]\n",
    "policy = random_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39054219, 0.09620872, 0.12107623, 0.39217285],\n",
       "       [0.33746006, 0.26317891, 0.25758786, 0.14177316],\n",
       "       [0.42045455, 0.06818182, 0.27272727, 0.23863636],\n",
       "       [0.06197965, 0.10823312, 0.32192414, 0.50786309],\n",
       "       [0.09431525, 0.30706288, 0.17269595, 0.42592593],\n",
       "       [0.02374937, 0.41687721, 0.09247094, 0.46690248],\n",
       "       [0.38274336, 0.00088496, 0.44159292, 0.17477876],\n",
       "       [0.08606557, 0.22540984, 0.32172131, 0.36680328],\n",
       "       [0.20946822, 0.61673152, 0.03696498, 0.13683528],\n",
       "       [0.14633106, 0.36348123, 0.4112628 , 0.07892491],\n",
       "       [0.42186002, 0.19654842, 0.06040268, 0.32118888],\n",
       "       [0.23526547, 0.22747199, 0.17584023, 0.36142231],\n",
       "       [0.35245902, 0.38196721, 0.07431694, 0.19125683],\n",
       "       [0.20588235, 0.37591912, 0.13143382, 0.28676471],\n",
       "       [0.28764569, 0.2027972 , 0.13286713, 0.37668998],\n",
       "       [0.3       , 0.3496    , 0.2944    , 0.056     ],\n",
       "       [0.19237013, 0.02272727, 0.12175325, 0.66314935],\n",
       "       [0.03219697, 0.27020202, 0.47032828, 0.22727273],\n",
       "       [0.21711457, 0.2029703 , 0.33698727, 0.24292786],\n",
       "       [0.18553689, 0.31848064, 0.30679328, 0.18918919],\n",
       "       [0.17487685, 0.03793103, 0.38423645, 0.40295567],\n",
       "       [0.0626615 , 0.21640827, 0.14857881, 0.57235142],\n",
       "       [0.0641635 , 0.36311787, 0.24477186, 0.32794677],\n",
       "       [0.43800322, 0.25925926, 0.27589909, 0.02683843],\n",
       "       [0.06405124, 0.46116894, 0.34907926, 0.12570056]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random policy\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Episode following Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(steps):\n",
    "\n",
    "    # set initial state\n",
    "    state = grid.initial_state()\n",
    "\n",
    "    # initialize state (with iniitial state), action list and reward list\n",
    "    state_list = [state]\n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "\n",
    "    # generate an episode\n",
    "    for i in range(steps):\n",
    "\n",
    "        # pick an action based on categorical distribution in policy\n",
    "        action = int(np.random.choice(action_count, 1, p=policy[grid.states.index(state)])) # get index in int\n",
    "        action = actions[action] # convert the integer index (ie. 0) to action (ie. [-1, 0])\n",
    "\n",
    "        # get reward (integer value)\n",
    "        reward = grid.reward(state, action)\n",
    "\n",
    "        # get the new state with the chosen action\n",
    "        new_state = list(grid.p_transition(state, action)) # (ie. [1,0])\n",
    "        state = new_state # set the next state as the current state\n",
    "\n",
    "        # save state, action chosen and reward to list\n",
    "        state_list.append(state)\n",
    "        action_list.append(action)\n",
    "        reward_list.append(reward)\n",
    "        \n",
    "    return state_list, action_list, reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TESTING PURPOSE ################################################################################\n",
    "\n",
    "# # set initial state\n",
    "# state = grid.initial_state()\n",
    "\n",
    "# # initialize state (with iniitial state), action list and reward list\n",
    "# state_list = [state]\n",
    "# action_list = []\n",
    "# reward_list = []\n",
    "\n",
    "# # generate an episode\n",
    "# for i in range(200):\n",
    "\n",
    "#     # pick an action based on categorical distribution in policy\n",
    "#     action = int(np.random.choice(action_count, 1, p=policy[grid.states.index(state)])) # get index in int\n",
    "#     action = actions[action] # convert the integer index (ie. 0) to action (ie. [-1, 0])\n",
    "\n",
    "#     # get reward (integer value)\n",
    "#     reward = grid.reward(state, action)\n",
    "\n",
    "#     # get the new state with the chosen action\n",
    "#     new_state = list(grid.p_transition(state, action)) # (ie. [1,0])\n",
    "#     state = new_state # set the next state as the current state\n",
    "\n",
    "#     # save state, action chosen and reward to list\n",
    "#     state_list.append(state)\n",
    "#     action_list.append(action)\n",
    "#     reward_list.append(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Visit MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize q values for all state action pairs\n",
    "Q_values = np.zeros((state_count, action_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize parameters\n",
    "gamma = 0.99\n",
    "epsilon = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define average function\n",
    "def Average(lst): \n",
    "    return sum(lst) / len(lst) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate 500 times: each time, generating an episode of 200 steps\n",
    "max_steps = 200\n",
    "\n",
    "# define variables for keeping track of time steps\n",
    "Terminal = max_steps\n",
    "t_list=[]\n",
    "for i in range(1,max_steps+1):\n",
    "    t = Terminal - i\n",
    "    t_list.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration 500 times\n",
    "for iteration in range(500):\n",
    "  \n",
    "    # generate an episode of specified step count\n",
    "    state_list, action_list, reward_list = generate_episode(max_steps)\n",
    "    \n",
    "#     print(\"state_list: \", state_list)\n",
    "#     print(\"action_list: \", action_list)\n",
    "#     print(\"reward_list: \", reward_list)\n",
    "    \n",
    "    # intialize G\n",
    "    G = 0\n",
    "    \n",
    "    # initiate returns and visited list to none\n",
    "    returns_list = []\n",
    "    visited_list = []\n",
    "\n",
    "    # loop for each step of episode: T-1, T-2, T-3 ... 0 = 199, 198, 197 ... 0\n",
    "    for t in t_list:\n",
    "\n",
    "        # calculate G: starting with the last reward at index t (naturally accounts for pseudocode's \"t-1\")\n",
    "        G = gamma*G + reward_list[t]\n",
    "        # print(G)\n",
    "        \n",
    "        # combine state action pair, for example, state = [0,0], action = [0,1], state_action_pair = [0,0,0,1]\n",
    "        state_action_pair = []\n",
    "        state_action_pair.extend(state_list[t])\n",
    "        state_action_pair.extend(action_list[t])\n",
    "        # print(state_action_pair)\n",
    "\n",
    "        # check if state action pair have been visited before (if not: continue, else: move to the next time step)\n",
    "        if state_action_pair not in visited_list:\n",
    "\n",
    "            # add state action pair to visited list\n",
    "            visited_list.append(state_action_pair)\n",
    "\n",
    "            # append G to returns\n",
    "            returns_list.append(G)\n",
    "\n",
    "            # find state and action index, for example, converting action [-1, 0] to 0, and same for state #\n",
    "            state_index = grid.states.index(state_list[t])\n",
    "            action_index = actions.index(action_list[t])\n",
    "    #         print(\"state_index: \", state_index)\n",
    "\n",
    "            # write Q_values to the state-action pair\n",
    "            Q_values[state_index][action_index] = Average(returns_list)\n",
    "    #         print(\"average return: \", Average(returns_list))\n",
    "\n",
    "            # choose best action at given state\n",
    "            choose_action = np.argmax(Q_values[state_index])\n",
    "\n",
    "            # overwrite policy\n",
    "            for a in range(action_count): # for action in actions [0, 1, 2, 3]\n",
    "                if choose_action == a: # if the choose_action is the same as the current action\n",
    "                    policy[state_index][a] = 1 - epsilon + epsilon/action_count \n",
    "                else: # if choose_action is not the same as the current action\n",
    "                    policy[state_index][a] = epsilon/action_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# total unique state action pairs at the end of one episode\n",
    "print(len(visited_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.02150053,   1.32064996,   1.49101055,   2.33017804],\n",
       "       [  0.83333333, -10.02601781, -13.58036982,   0.20616999],\n",
       "       [ -5.43687306,  -0.37283809,   0.        ,   1.53076923],\n",
       "       [ -0.43087008,   0.70406367, -10.56168644,  -9.74178292],\n",
       "       [ -1.        , -10.65066812,  -2.283798  ,  -9.5576847 ],\n",
       "       [  1.86748756,   0.        , -23.57679877, -21.03122514],\n",
       "       [ -2.09112926,   0.        ,   0.        ,  -0.995     ],\n",
       "       [  2.1215    ,  -6.80868351,  -1.60789912,   0.        ],\n",
       "       [ -0.16612124,  -5.12893186,  -0.71658604,  -1.39823648],\n",
       "       [ -1.9701995 , -10.55831937, -10.98884684, -12.3451642 ],\n",
       "       [  0.7997181 ,  -0.27206614,  -1.02687366,  -1.37653349],\n",
       "       [ -1.83738911,   0.        ,   1.33827555,   0.22001257],\n",
       "       [  0.        ,  -5.58444054,  -1.46383825,  -1.67208174],\n",
       "       [ -1.20609929,  -0.86683861,  -4.98068988,  -5.83707884],\n",
       "       [ -9.86397351, -11.50299892, -11.33214899,  -1.02952684],\n",
       "       [ -0.67674423,  -7.40094773,  -2.80490375,  -3.341687  ],\n",
       "       [  0.        ,   0.        ,  -1.23012475,   0.        ],\n",
       "       [ -1.63223254,   1.0441433 ,   0.        ,   0.        ],\n",
       "       [ -5.30563413,  -9.50030151,  -9.12009667,  -4.62033777],\n",
       "       [-10.11683574,  -8.66361303,  -7.80266711, -11.61967346],\n",
       "       [ -3.81829004,  -2.25666069,  -1.        ,  -0.5       ],\n",
       "       [ -1.3702788 ,   0.        ,  -0.09090909,  -4.21945746],\n",
       "       [  0.        ,  -1.26838753, -10.16630576,  -0.5661221 ],\n",
       "       [-18.61483573, -17.26114901,  -5.54675566,  -0.94638408],\n",
       "       [-13.34922214,  -6.42601257,  -6.90516274,  -6.12646122]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05, 0.05, 0.05, 0.85],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.05, 0.05, 0.05, 0.85],\n",
       "       [0.05, 0.85, 0.05, 0.05],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.05, 0.85, 0.05, 0.05],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.05, 0.05, 0.85, 0.05],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.05, 0.85, 0.05, 0.05],\n",
       "       [0.05, 0.05, 0.05, 0.85],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.05, 0.85, 0.05, 0.05],\n",
       "       [0.05, 0.05, 0.05, 0.85],\n",
       "       [0.05, 0.05, 0.85, 0.05],\n",
       "       [0.05, 0.05, 0.05, 0.85],\n",
       "       [0.05, 0.85, 0.05, 0.05],\n",
       "       [0.85, 0.05, 0.05, 0.05],\n",
       "       [0.05, 0.05, 0.05, 0.85],\n",
       "       [0.05, 0.05, 0.05, 0.85]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#up, right, down, left = (clockwise from up) \n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Table: \n",
      "      0      1      2      3     4\n",
      "0  left     up   left  right    up\n",
      "1    up  right     up     up    up\n",
      "2    up   down     up  right  left\n",
      "3    up     up  right   left  down\n",
      "4  left  right     up   left  left\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PRINT POLICY TABLE ################################################################################\n",
    "# import pandas library\n",
    "import pandas as pd\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)\n",
    "\n",
    "# iterate through policy to make a table that represents action number\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(len(policy)):\n",
    "    \n",
    "    # find the best action at each state\n",
    "    best_action = np.argmax(policy[state])\n",
    "\n",
    "    # get action name\n",
    "    if best_action == 0:\n",
    "        action_name = 'up'\n",
    "    elif best_action == 1:\n",
    "        action_name = 'right'\n",
    "    elif best_action == 2:\n",
    "        action_name = 'down'\n",
    "    else:\n",
    "        action_name = 'left'\n",
    "\n",
    "    # calculate the row and column coordinate of the current state number\n",
    "    row = int(state/grid.size)\n",
    "    column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "            \n",
    "    # assign action name\n",
    "    policy_table.loc[row][column] = action_name\n",
    "\n",
    "print(\"Policy Table: \")\n",
    "print(policy_table)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
